import asyncio
import os
from dotenv import load_dotenv
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

load_dotenv()

class FastMCPTestClient:
    """
    æµ‹è¯• FastMCP æœåŠ¡å™¨çš„å®¢æˆ·ç«¯
    """
    def __init__(self, model: str = "gpt-4o"):
        self.llm = ChatOpenAI(
            model=model,
            temperature=0,
            api_key=os.getenv("API_KEY"),
            base_url=os.getenv("BASE_URL")
        )
        self.model = model
        self.session = None

    async def connect_to_server(self, server_script_path: str = "fastmcp_server.py"):
        """
        è¿æ¥åˆ° FastMCP æœåŠ¡å™¨
        """
        server_params = StdioServerParameters(
            command="python",
            args=[server_script_path],
        )
        self.stdio_client_ctx = stdio_client(server_params)
        self.stdio = await self.stdio_client_ctx.__aenter__()
        self.read_stream, self.write_stream = self.stdio
        self.session_ctx = ClientSession(self.read_stream, self.write_stream)
        self.session = await self.session_ctx.__aenter__()
        await self.session.initialize()
        
        tools_result = await self.session.list_tools()
        print("\nğŸ”— Connected to FastMCP server with tools:")
        for tool in tools_result.tools:
            print(f"  - {tool.name}: {tool.description}")

    async def get_mcp_tools(self):
        """è·å– MCP å·¥å…·åˆ—è¡¨"""
        tools_result = await self.session.list_tools()
        return [
            {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description or "",
                    "parameters": tool.inputSchema
                }
            }
            for tool in tools_result.tools
        ]

    async def process_query(self, query: str) -> str:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼Œè®© LLM é€‰æ‹©åˆé€‚çš„å·¥å…·
        """
        tools = await self.get_mcp_tools()
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å„ç§å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ã€‚æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚é€‰æ‹©åˆé€‚çš„å·¥å…·ã€‚"),
            HumanMessage(content=query)
        ]
        
        resp = self.llm.invoke(messages, tools=tools, tool_choice="auto")
        print(f"\nğŸ¤– LLM Response: {resp}")
        print("-" * 80)
        
        tool_results = []
        if hasattr(resp, "tool_calls") and resp.tool_calls:
            for tool_call in resp.tool_calls:
                tool_name = tool_call["name"]
                args = tool_call["args"]
                print(f"ğŸ”§ Calling tool: {tool_name} with args: {args}")
                
                tool_result = await self.session.call_tool(tool_name, args)
                result_text = tool_result.content[0].text
                print(f"ğŸ“‹ Tool {tool_name} result: {result_text}")
                tool_results.append((tool_name, result_text))
            
            # è®© LLM æ€»ç»“å·¥å…·ç»“æœ
            tool_results_str = "\n".join(
                [f"å·¥å…· {name} çš„ç»“æœï¼š\n{result}" for name, result in tool_results]
            )
            final_messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå–„äºæ€»ç»“å’Œè§£é‡Šçš„AIåŠ©æ‰‹ã€‚è¯·æ ¹æ®å·¥å…·è¿”å›çš„ç»“æœï¼Œä¸ºç”¨æˆ·æä¾›æ¸…æ™°ã€æœ‰ç”¨çš„å›ç­”ã€‚"),
                HumanMessage(content=f"ç”¨æˆ·é—®é¢˜ï¼š{query}\n\nå·¥å…·æ‰§è¡Œç»“æœï¼š\n{tool_results_str}")
            ]
            final = self.llm.invoke(final_messages)
            print(f"\nğŸ’¡ Final Answer: {final.content}")
            return final.content
        else:
            print(f"ğŸ’¬ Direct Answer: {resp.content}")
            return resp.content

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        await self.session_ctx.__aexit__(None, None, None)
        await self.stdio_client_ctx.__aexit__(None, None, None)

async def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå„ç§åŠŸèƒ½"""
    client = FastMCPTestClient()
    
    try:
        await client.connect_to_server("fastmcp_server.py")
        
        # æµ‹è¯•ç”¨ä¾‹
        test_queries = [
            "What is our company's vacation policy?",
            "Show me the current directory contents",
            "What's the system information?",
            "Calculate 2 + 3 * 4",
            "Create a test file with some content",
            "What's the current working directory?",
            "Show me information about the data/kb.json file"
        ]
        
        for i, query in enumerate(test_queries, 1):
            print(f"\n{'='*80}")
            print(f"ğŸ§ª Test {i}: {query}")
            print(f"{'='*80}")
            
            try:
                response = await client.process_query(query)
                print(f"\nâœ… Test {i} completed successfully!")
            except Exception as e:
                print(f"\nâŒ Test {i} failed: {str(e)}")
            
            print("\n" + "-"*80)
    
    finally:
        await client.cleanup()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except RuntimeError as e:
        if "Attempted to exit cancel scope" in str(e):
            print("Warning: Cancel scope exit order issue (can be ignored if all results above are correct).")
        else:
            raise 